# Default values for nomad.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
roll: false
nameOverride: ''
fullnameOverride: ''

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ''

nomad:
  enabled: true

  config:
    services:
      api_host: localhost
      api_base_path: /nomad-oasis
      api_port: 80
      https: false
      upload_limit: 10
      admin_user_id: ""

    fs:
      staging_external: /nomad/staging
      public_external: /nomad/public
      north_home_external: /nomad/north/users
      tmp: /nomad/tmp
      prefix_size: 1
      archive_version_suffix: []
      working_directory: /app
      nomad: /nomad

    mongo:
      db_name: nomad_oasis
      port: 27017

    elastic:
      port: 9200
      timeout: 60
      bulk_timeout: 600
      bulk_size: 1000
      entries_per_material_cap: 1000

    logstash:
      enabled: false
      tcp_port: 5000

    temporal:
      enabled: false
      namespace: default
      graceful_shutdown_timeout: null
      processing_timeouts: {}

    meta:
      service: app
      homepage: https://nomad-lab.eu
      source_url: https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-distro
      maintainer_email: support@nomad-lab.eu
      footer_links: []
      beta:
        label: ""
        isTest: false
        isBeta: false
        usesBetaData: false
        officialUrl: ""

    process:
      reuse_parser: true
      index_materials: true
      rfc3161_skip_published: false

    reprocess:
      rematch_published: true
      reprocess_existing_entries: true
      use_original_parser: false
      add_matched_entries_to_published: false
      delete_unmatched_published_entries: false
      index_individual_entries: false

    celery:
      routing: queue
      timeout: 7200
      acks_late: false

    logging:
      app:
        console_loglevel: INFO
        logstash_loglevel: INFO
      worker:
        console_loglevel: ERROR
        logstash_loglevel: INFO

    mail:
      enabled: false
      host: localhost
      port: 25
      from_address: support@nomad-lab.eu
      user: ""
      password: ""
      cc_address: null

    client:
      user: admin

    keycloak:
      server_url: ""
      realm_name: ""
      username: admin
      client_id: ""

    datacite:
      enabled: false
      prefix: "10.17172"

    north:
      enabled: false
      hub_service_api_token: secret-token
      tools: {}

    gui:
      debug: false
      gzip: true
      config: {}

    oasis:
      is_oasis: true
      uses_central_user_management: false

    # plugins: {}
    # normalize: {}
    # archive: {}
    # ui: {}

  image:
    repository: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-distro
    tag: latest
    pullPolicy: Always

  imagePullSecrets: []

  infrastructure:
    # MongoDB settings (host defaults to {{ .Release.Name }}-mongodb if empty)
    mongo:
      host: ""

    # Elasticsearch settings (host defaults to elasticsearch-master if empty)
    elastic:
      host: ""

    # RabbitMQ (host defaults to {{ .Release.Name }}-rabbitmq if empty)
    rabbitmq:
      host: ""

    # Logstash settings
    logstash:
      host: ""

  secrets:
    # API secret for cryptographic operations
    api:
      existingSecret: ""
      key: "password"
      value: ""
      autoGenerate: true

    # Keycloak secrets
    keycloak:
      clientSecret:
        existingSecret: ""
        key: "password"
        value: ""
      password:
        existingSecret: ""
        key: "password"
        value: ""

    # Client password
    client:
      password:
        existingSecret: ""
        key: "password"
        value: ""

    # DataCite credentials
    datacite:
      existingSecret: ""
      key: "password"
      value: ""

    # JupyterHub service API token
    north:
      hubServiceApiToken:
        existingSecret: ""
        key: "token"
        value: ""

  proxy:
    replicaCount: 1
    timeout: 60
    editTimeout: 60
    connectionTimeout: 10
    image:
      repository: nginx
      tag: 1.27.4-alpine
      pullPolicy: IfNotPresent
    command: [nginx]
    args: [-g, daemon off;]
    imagePullSecrets: []
    service:
      type: ClusterIP
      port: 80
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    volumes: []
    volumeMounts: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    podLabels: {}

  app:
    replicaCount: 1
    service:
      type: ClusterIP
      port: 8000
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    volumes: []
    volumeMounts: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    podLabels: {}


  worker:
    replicaCount: 1
    terminationGracePeriodSeconds: 300
    processes: null
    maxTasksPerChild: 128
    storage: null  # "memory" or "disk"
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 10
      cooldownPeriod: 1200
      initialCooldownPeriod: 600
      targetQueueSize: 5
      temporal:
        taskQueue: nomad-internal-workflows-task-queue
        queueTypes: workflow,activity
      advanced:
        horizontalPodAutoscalerConfig:
          behavior:
            scaleDown:
              stabilizationWindowSeconds: 1800
              policies:
              - type: Percent
                value: 25
                periodSeconds: 600
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    volumes: []
    volumeMounts: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    podLabels: {}

  gpuworker:
    replicaCount: 0
    image:
      repository: ""
      tag: ""
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    volumes: []
    volumeMounts: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    podLabels: {}

  cpuworker:
    replicaCount: 0
    image:
      repository: ""
      tag: ""
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    volumes: []
    volumeMounts: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    podLabels: {}

  adminconsole:
    enabled: false
    replicaCount: 1
    podSecurityContext: {}
    securityContext: {}
    resources: {}
    volumes: []
    volumeMounts: []
    nodeSelector: {}
    tolerations: []
    affinity: {}
    podAnnotations: {}
    podLabels: {}

  ingress:
    enabled: false
    limitConnections: 32
    limitConnectionsApi: 8
    className: ''
    annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: 'false'
      nginx.ingress.kubernetes.io/proxy-body-size: 32g
    tls: []

  volumes: []
  volumeMounts: []
  service:
    type: ClusterIP
    port: 80

jupyterhub:
  enabled: false
  debug:
    enabled: false
  proxy:
    service:
      type: ClusterIP
  singleuser:
    image:
      pullPolicy: Always
    storage:
      type: none
    # The networkPolicy allows us to block part of the outgoing traffic to prevent.
    # source: https://github.com/jupyterhub/mybinder.org-deploy/blob/main/mybinder/values.yaml#L104
    networkPolicy:
      enabled: true
      egress:
      - to:
        - ipBlock:
            cidr: 0.0.0.0/0
        ports:
        - protocol: TCP
          port: 80 # http
        - protocol: TCP
          port: 443 # https
        - protocol: TCP
          port: 873 # rsync
        - protocol: TCP
          port: 1094 # xroot
        - protocol: TCP
          port: 1095 # xroot
        - protocol: TCP
          port: 4001 # IPFS
        - protocol: TCP
          port: 9418 # git
        - protocol: TCP
          port: 16286 # Wolfram Engine on-demand licensing
      egressAllowRules:
        # It is important to disable nonPrivateIPs otherwise the networkPolicy's will be silently ignored
        nonPrivateIPs: false
  hub:
    extraEnv:
      NOMAD_NORTH_HUB_SERVICE_API_TOKEN:
        valueFrom:
          secretKeyRef:
            name: nomad-hub-service-api-token
            key: token
    allowNamedServers: true
    shutdownOnLogout: true
    config:
      JupyterHub:
        authenticator_class: generic-oauth
      Authenticator:
        auto_login: true
        enable_auth_state: true
      GenericOAuthenticator:
        client_id: nomad_public
        oauth_callback_url:
        authorize_url:
          https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/auth
        token_url:
          https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/token
        userdata_url:
          https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/userinfo
        login_service: keycloak
        username_key: preferred_username
        userdata_params:
          state: state
    extraConfig:
      01-prespawn-hook.py: |
        import os
        import requests
        import asyncio

        hub_service_api_token = os.getenv('NOMAD_NORTH_HUB_SERVICE_API_TOKEN')

        # configure nomad service
        c.JupyterHub.services.append(
            {
                "name": "nomad-service",
                "admin": True,
                "api_token": hub_service_api_token,
            }
        )

        async def pre_spawn_hook(spawner):
            await spawner.load_user_options()
            username = spawner.user.name

            spawner.log.info(f"username: {username}")
            spawner.log.debug(f'Configuring spawner for named server {spawner.name}')

            if spawner.handler.current_user.name != 'nomad-service':
                # Do nothing, will only launch the default image with no volumes.
                return

            user_home = spawner.user_options.get('user_home')
            spawner.log.info(f"user_home: {user_home}")
            if user_home:
                spawner.volumes.append({
                    'name': 'user-home',
                    'hostPath': {'path': user_home['host_path']}
                })
                spawner.volume_mounts.append({
                    'name': 'user-home',
                    'mountPath': user_home['mount_path'],
                    'readOnly': False
                })

            uploads = spawner.user_options.get('uploads', [])
            spawner.log.info(f"uploads: {uploads}")
            for (i, upload) in enumerate(uploads):
                spawner.volumes.append({
                    'name': f"uploads-{i}",
                    'hostPath': {'path': upload['host_path']}
                })
                spawner.volume_mounts.append({
                    'name': f"uploads-{i}",
                    'mountPath': upload['mount_path'],
                    'readOnly': False
                })

            environment = spawner.user_options.get('environment', {})
            spawner.environment.update(environment)

            tool = spawner.user_options.get('tool')
            if tool:
                spawner.image = tool.get('image')
                spawner.cmd = tool.get('cmd')

                # Workaround to have specific default_url for specific containers without using profiles
                if tool.get('default_url'):
                  spawner.default_url = tool.get('default_url')

                # Workaround for webtop based images (no connection to jupyterhub itself)
                if tool.get('privileged'):
                    spawner.privileged = True
                    spawner.allow_privilege_escalation = True
                    spawner.uid = 0

        c.Spawner.pre_spawn_hook = pre_spawn_hook
        c.OAuthenticator.allow_all = True

  cull:
    enabled: true
    timeout: 86400 # 24 hours
    every: 600
    removeNamedServers: true

  prePuller:
    hook:
      enabled: true
      image:
        pullPolicy: Always
    continuous:
      enabled: false
    extraImages: {}
  scheduling:
    userScheduler:
      enabled: false
    podPriority:
      enabled: false
    userPlaceholder:
      enabled: false
      replicas: 0

mongodb:
  enabled: true

elasticsearch:
  enabled: true

temporal:
  enabled: false
  schema:
    createDatabase:
      enabled: false
    setup:
      enabled: true
    update:
      enabled: true
  cassandra:
    enabled: false
  elasticsearch:
    enabled: false
  prometheus:
    enabled: false
  grafana:
    enabled: false
  web:
    enabled: true
    replicaCount: 1
    service:
      type: ClusterIP

  server:
    replicaCount: 1
    config:
      namespaces:
        create: true
      persistence:
        default:
          driver: sql
          sql:
            driver: postgres12
            port: 5432
            database: temporal
            user: temporal
            password: temporal
            maxConns: 20
            maxIdleConns: 20
            maxConnLifetime: 1h

        visibility:
          driver: sql
          sql:
            driver: postgres12
            port: 5432
            database: temporal_visibility
            user: temporal
            password: temporal
            maxConns: 20
            maxIdleConns: 20
            maxConnLifetime: 1h

postgresql:
  enabled: false
  image:
    repository: bitnamilegacy/postgresql
  global:
    postgresql:
      auth:
        postgresPassword: postgres
        username: temporal
        password: temporal
  primary:
    initdb:
      scripts:
        01-init.sql: |-
          CREATE DATABASE "temporal";
          CREATE DATABASE "temporal_visibility";

keda:
  enabled: false
